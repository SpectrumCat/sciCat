{"meta":{"title":"SciCat","subtitle":"Ad astra per aspera","description":"","author":"Spectrum Cat","url":"http://example.com","root":"/"},"pages":[],"posts":[{"title":"【Benchmark】LVIS minival 开放词汇检测","slug":"【Benchmark】LVIS minival 开放词汇检测","date":"2025-02-21T00:33:25.000Z","updated":"2025-02-21T08:49:30.923Z","comments":true,"path":"2025/02/21/【Benchmark】LVIS minival 开放词汇检测/","permalink":"http://example.com/2025/02/21/%E3%80%90Benchmark%E3%80%91LVIS%20minival%20%E5%BC%80%E6%94%BE%E8%AF%8D%E6%B1%87%E6%A3%80%E6%B5%8B/","excerpt":"","text":"方法 在LVIS上预训练 文本来源 时间 mAP for rare Grounding DINO-L 否 BERT 2023 33.9 DetCLIP-L（无代码） 否 GPT-4 2024 45.1 Yolo-world-L 否 CLIP 2024 35.4 CP-DETR-Pro（无代码） 是 CLIP 2024 47.6 (not rare) DITO 是 CLIP 2024 40.5 LaMI-DETR 是 GPT-3.5 2024 43.4","categories":[],"tags":[{"name":"Benchmark","slug":"Benchmark","permalink":"http://example.com/tags/Benchmark/"},{"name":"LVIS minival","slug":"LVIS-minival","permalink":"http://example.com/tags/LVIS-minival/"},{"name":"OVD","slug":"OVD","permalink":"http://example.com/tags/OVD/"}]},{"title":"【每日阅读】YOLO-World_ Real-Time Open-Vocabulary Object Detection","slug":"【每日阅读】Yolo-World","date":"2025-02-21T00:25:14.000Z","updated":"2025-02-21T08:29:29.761Z","comments":true,"path":"2025/02/21/【每日阅读】Yolo-World/","permalink":"http://example.com/2025/02/21/%E3%80%90%E6%AF%8F%E6%97%A5%E9%98%85%E8%AF%BB%E3%80%91Yolo-World/","excerpt":"","text":"1.Abstract&amp;Info1.1 AbstractThe You Only Look Once (YOLO) series of detectors have established themselves as efficient and practical tools. However, their reliance on predefined and trained object categories limits their applicability in open scenarios. Addressing this limitation, we introduce YOLO-World, an innovative approach that enhances YOLO with open-vocabulary detection capabilities through vision-language modeling and pre-training on large-scale datasets. Specifically, we propose a new Re-parameterizable Vision-Language Path Aggregation Network (RepVL-PAN) and region-text contrastive loss to facilitate the interaction between visual and linguistic information. Our method excels in detecting a wide range of objects in a zero-shot manner with high efficiency. On the challenging LVIS dataset, YOLO-World achieves 35.4 AP with 52.0 FPS on V100, which outperforms many state-of-the-art methods in terms of both accuracy and speed. Furthermore, the fine-tuned YOLO-World achieves remarkable performance on several downstream tasks, including object detection and open-vocabulary instance segmentation. 1.2 InfoAuthors: Tianheng Cheng, Lin Song, Yixiao Ge, Wenyu Liu, Xinggang Wang, Ying ShanDOI: 10.48550&#x2F;arXiv.2401.17270Publication:Data: 2024-02-22 2. Annotation%% begin annotations %% Imported: 2025-02-21 4:24 下午原文：[Re-parameterizable VisionLanguage Path Aggregation Network]标注：核心 原文：[for high-efficiency open-vocabulary object detection]标注：重点在高效 原文：[YOLO-World follows the standard YOLO architecture [20] and leverages the pre-trained CLIP [39] text encoder to encode the input texts.]标注：使用CLIP作为文本信息的提供方 原文：[During inference, the text encoder can be removed and the text embeddings can be re-parameterized into weights of RepVL-PAN for efficient deployment.]标注：训练时将文本信息转入RepVL-PAN，运行时不再依赖CLIP 原文：[the prompt-thendetect paradigm (Fig. 2 (c)) first encodes the prompts of a user to build an offline vocabulary and the vocabulary varies with different needs.]标注：首先将输入的文本转化为提示，同一词汇在不同场景下产生的提示可能不同 原文：[which consists of a YOLO detector, a Text Encoder, and a Re-parameterizable Vision-Language Path Aggregation Network (RepVL-PAN).]标注：YoloWorld的三个部分 原文：[YOLO Detector. YOLO-World is mainly developed based on YOLOv8 [20], which contains a Darknet backbone [20, 43] as the image encoder, a path aggregation network (PAN) for multi-scale feature pyramids, and a head for bounding box regression and object embeddings.]标注：Yolo检测器构成 原文：[Given the text T , we adopt the Transformer text encoder pre-trained by CLIP [39] to extract the corresponding text embeddings W &#x3D; TextEncoder(T ) ∈ RC×D,]标注： 原文：[we adopt the decoupled head with two 3×3 convs to regress bounding boxes {bk}K k&#x3D;1 and object embeddings {ek}K k&#x3D;1,]标注：两个解耦头，分别负责分类和定位回归 原文：[We present a text contrastive head to obtain the object-text similarity sk,j by]标注：通过公式1进行解耦 原文：[During training, we construct an online vocabulary T for each mosaic sample containing 4 images]标注：以马赛克的方式进行训练 原文：[the user can define a series of custom prompts, which might include captions or categories.]标注：离线运行时使用预定的prompt进行embedding 原文：[we propose the Text-guided CSPLayer (T-CSPLayer) and Image-Pooling Attention (I-Pooling Attention)]标注：RepVL-PAN的组成 原文：[we adopt the max-sigmoid attention after the last dark bottleneck block to aggregate text features into image features by:]标注：公式2即如何将文本信息与视觉信息融合的方法 原文：[To enhance the text embeddings with image-aware information, we aggregate image features to update the text embeddings by proposing the Image-Pooling Attention. Rather than directly using crossattention on image features,]标注：公式3为如何将视觉信息融合入文本信息 原文：[LVIS minival]标注：LVIS数据集是迁移过来的，应用在minival上 原文：[Table 3. Ablations on Pre-training Data. We evaluate the zeroshot performance on LVIS of pre-training YOLO-World with different amounts of data.]标注：在添加CC3M之后情况更好，但需要额外的数据处理 原文：[Table 5. Text Encoder in YOLO-World. We ablate different text encoders in YOLO-World through the zero-shot LVIS evaluation]标注：如果采用CLIP，frozen比较好 %% end annotations %% %% Import Date: 2025-02-21T16:24:37.343+08:00 %%","categories":[],"tags":[{"name":"文献阅读","slug":"文献阅读","permalink":"http://example.com/tags/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB/"}]},{"title":"【每日阅读】Grounding DINO_ Marrying DINO with Grounded Pre-Training for Open-Set Object Detection","slug":"【每日阅读】Grounding DINO","date":"2025-02-20T19:17:11.000Z","updated":"2025-02-21T03:31:34.365Z","comments":true,"path":"2025/02/21/【每日阅读】Grounding DINO/","permalink":"http://example.com/2025/02/21/%E3%80%90%E6%AF%8F%E6%97%A5%E9%98%85%E8%AF%BB%E3%80%91Grounding%20DINO/","excerpt":"","text":"1.Abstract&amp;Info1.1 AbstractIn this paper, we present an open-set object detector, called Grounding DINO, by marrying Transformer-based detector DINO with grounded pre-training, which can detect arbitrary objects with human inputs such as category names or referring expressions. The key solution of open-set object detection is introducing language to a closed-set detector for open-set concept generalization. To effectively fuse language and vision modalities, we conceptually divide a closed-set detector into three phases and propose a tight fusion solution, which includes a feature enhancer, a language-guided query selection, and a cross-modality decoder for cross-modality fusion. While previous works mainly evaluate open-set object detection on novel categories, we propose to also perform evaluations on referring expression comprehension for objects specified with attributes. Grounding DINO performs remarkably well on all three settings, including benchmarks on COCO, LVIS, ODinW, and RefCOCO&#x2F;+&#x2F;g. Grounding DINO achieves a $52.5$ AP on the COCO detection zero-shot transfer benchmark, i.e., without any training data from COCO. It sets a new record on the ODinW zero-shot benchmark with a mean $26.1$ AP. Code will be available at \\url{https://github.com/IDEA-Research/GroundingDINO}. 1.2 InfoAuthors: Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Qing Jiang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, Lei ZhangDOI: 10.48550&#x2F;ARXIV.2303.05499Publication:Data: 2023-01-01 2. Annotation%% begin annotations %% Imported: 2025-02-17 1:02 下午原文：[Feature fusion can be performed in three phases: neck (phase A), query initialization (phase B), and head (phase C). For example, GLIP [25] performs early fusion in the neck module (phase A), and OV-DETR [55] uses language-aware queries as head inputs (phase B). We argue that introducing more feature fusion into the pipeline can facilitate better alignment between different modality features, thereby achieving better performance.]标注：文章认为在三个部分可以加入语义信息，越多地方加就越好 原文：[Although conceptually simple, it is hard for previous work to perform feature fusion in all three phases.]标注：同时在三个部分加入比较困难 原文：[Unlike classical detectors, the Transformer-based detector method such as DINO has a consistent structure with language blocks.]标注：相比基于CNN的方法，基于DETR的方法更好，因为其结构与语言模型相似 原文：[Most existing open-set models [14, 21] rely on pre-trained CLIP models for concept generalization]标注：第一种方法，基于CLIP做概念泛化 原文：[GLIP [25] presents a different way by reformulating object detection as a phrase grounding task and introducing contrastive training between object regions and language phrases on large-scale data.]标注：第二种，将目标检测视为描述grounding任务，在物体区域和描述之间建立联系 原文：[GLIP’s approach involves concatenating all categories into a sentence in a random order. However, the direct category names concatenation does not consider the potential influence of unrelated categories on each other when extracting features.]标注：GLIP方法的缺点 原文：[To mitigate this issue and improve model performance during grounded training, we introduce a technique that utilizes sub-sentence level text features.]标注：子句级别的语义特征 原文：[]标注：三个子设置：闭集目标检测，开集目标检测和提示目标检测 原文：[]标注： 原文：[INq &#x3D; TopNq (Max(−1)(XI X⊺ T )). (1)]标注：被选取的图像特征 原文：[Each cross-modality query is fed into a self-attention layer, an image cross-attention layer to combine image features, a text crossattention layer to combine text features, and an FFN layer in each cross-modality decoder layer.]标注：跨模态解码器，对query进行处理，注入文本信息 原文：[Table 2: Zero-shot domain transfer and fine-tuning on COCO. *]标注：zeroshot COCO结果 原文：[It eliminates the influence between different category names while keeping per-word features for fine-grained understanding.]标注：移除了不必要的类别名间影响 原文：[]标注：闭集：COCO 原文：[]标注：开集：zero-shot COCO, LVIS, ODinW 原文：[]标注：提示目标检测：RefCOCO&#x2F;+&#x2F;g 原文：[Using BERT as our text encoder,]标注：文本编码器：BERT 原文：[Table 3: Model results on LVIS.]标注：LVIS结果 原文：[Table 4: Model results on the ODinW benchmark.]标注：ODinW结果 %% end annotations %% %% Import Date: 2025-02-17T13:19:36.595+08:00 %%","categories":[],"tags":[{"name":"文献阅读","slug":"文献阅读","permalink":"http://example.com/tags/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB/"}]}],"categories":[],"tags":[{"name":"Benchmark","slug":"Benchmark","permalink":"http://example.com/tags/Benchmark/"},{"name":"LVIS minival","slug":"LVIS-minival","permalink":"http://example.com/tags/LVIS-minival/"},{"name":"OVD","slug":"OVD","permalink":"http://example.com/tags/OVD/"},{"name":"文献阅读","slug":"文献阅读","permalink":"http://example.com/tags/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB/"}]}